{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural network - Sequence Modelling \n",
    "\n",
    "Single Perceptron model - input, hidden state and output \n",
    "Sequence modelling - Sequence of single perceptrons \n",
    "\n",
    "it comes with time step - t \n",
    "\n",
    "we can express sequencing as yt=f(xt)  as these are not isolated and the prior information helps along with current time \n",
    "step input to derive the output. \n",
    "\n",
    "we can express as - yt=f(xt,ht-1)  where h(t-1) - prior information of time step t-1 \n",
    "\n",
    "This acts as recurrent state - So is called Recurrent Neural Network \n",
    "\n",
    "Examples: Stock price prediction, Image captioning and medical image processing, audio processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN from scratch\n",
    "class myRNNcell(tf.keras.layers.Layer):\n",
    "    def __init__(self,rnn_units,input_dim,output_dim):\n",
    "        super(myRNNcell,self).__init__()\n",
    "        \n",
    "        self.W_xh=self.add_weight([rnn_units,input_dim])\n",
    "        self.W_hh=self.add_weight([rnn_units,rnn_units])\n",
    "        self.W_hy=self.add_weight([output_dim,rnn_units])\n",
    "        \n",
    "        self.h=tf.zeros([rnn_units,1])\n",
    "        \n",
    "        \n",
    "    def call(self,x):\n",
    "        self.h=tf.math.tanh(self.W_hh * self.h + self.W_xh * x)\n",
    "        \n",
    "        output= self.W_hy * self.h\n",
    "        \n",
    "        return output,self.h\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tensorflow already provides implementation for Recurrent Neural Network\n",
    "\n",
    "We can just use the tf.keras.layers.SimpleRNN(rnn_units)  to create a recurrent nueral network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Sequence Modeling - Design Criteria\n",
    "\n",
    "1. Able to handle variables length sequences\n",
    "2. Track Long term dependencies \n",
    "3. Maintain information about sequence order \n",
    "4. Share parameters across the sequence \n",
    "\n",
    "Recurrent Neural Network follows all the sequence model criteria\n",
    "\n",
    "###### Note: Neural Network understand just numbers and outputs numbers \n",
    "\n",
    "Text representation into RNN - we have to encode the text into numeric vector before feeding into neural network\n",
    "\n",
    "#### Two ways of encoding\n",
    "* Hot embed encoding - finite sequenc of vector and each value is binary representing the word\n",
    "* Learned embedding \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training RNN \n",
    "\n",
    "Gradient Descent in RNN : Due to number of recurrent hidden nodes \n",
    "calculating gradient descent is complex and involves computational and due to which it might result in two problems\n",
    "\n",
    "* Exploding Gradients when many values>1:\n",
    "    \n",
    "        Gradient clipping when the values are so large we clip to avoid this problem\n",
    "    \n",
    "* Vanishing Gradients when many values<1 \n",
    "\n",
    "        1. Activation function - RelU activation function whose gradient does not shrink when value is grater compared\n",
    "           to tanh and sigmoid activation function\n",
    "           \n",
    "        2. Initialization of parameters - Initilization of wieghts to identity matrix to avoid random weight \n",
    "           initialization to zero\n",
    "           \n",
    "        3. Using complex gated cell which can controll information flow \n",
    "           LSTM - Long term short term memory \n",
    "           GRU \n",
    "           \n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM - Long term short term memory gated cell\n",
    "\n",
    "Consists of compuatation blocks which control information flow and can track information across throught many time steps.\n",
    "\n",
    "Tensorflow code:\n",
    "\n",
    "tf.keras.layers.LSTM(num_units) \n",
    "\n",
    "Gates- Information added or removed using gates\n",
    "       Example : Done using sigmoid and pointwise multiplication to control information flow \n",
    "       sigmoid - 0 to 1 - 0 no information and 1 all information\n",
    "       \n",
    "Forget - get rid of Irrelevant information \n",
    "Store - Relevant information from current input \n",
    "Update - (CFT) - cell state - selectively updated \n",
    "Output - controls what information will be sent to next step - filtered information of cell state\n",
    "\n",
    "By this LSTM helps RNN - from restoring long term dependencies and also avoid vanishing gradients.\n",
    "\n",
    "Allows uniterrupted Gradient flow - using state of CFT instead of HFT - backpropagation thru time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks - Applications\n",
    "\n",
    "1. Music Generation - input- sheet music output- next chapter in sheet music\n",
    "\n",
    "2. Sentiment Classification - Sequence of words - probability of having  positive sentiment \n",
    "\n",
    "    loss= tf.nn.softmax_cross_entropy_with_logits(y,predicted)\n",
    "    \n",
    "   tweet sentiment classification\n",
    "\n",
    "3. Machine Translation: input in one language and ouptu in other language\n",
    "\n",
    "   Encoder and Decoder\n",
    "   \n",
    "   Issues:\n",
    "   1. Encoding bottleneck\n",
    "   2. Slow, no parallelization as they are sequence models as it depends on previous history\n",
    "   3. Not long memory \n",
    "   \n",
    "   \n",
    "   Concept of Attention to overcome issues occur during longer sequences:\n",
    "   - Learnable memory access\n",
    "   - Large scale sequential models - transformers \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
