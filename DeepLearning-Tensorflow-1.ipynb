{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries required to demonstrate the deep neural nets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desne Layer from scratch\n",
    "class myDenseNet(tf.keras.layers.Layer):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(myDenseNet,self).__init__()\n",
    "        \n",
    "        self.W= self.add_weights([input_dim,output_dim])\n",
    "        self.b= self.add_weights([1,output_dim])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #forward propogate the inputs\n",
    "        z= tf.matmul(inputs, self.W) + self.b\n",
    "        output= tf.math.sigmoid(z)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a layer with for faster building of models\n",
    "layer= tf.keras.layers.Dense(units=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create sequential deep neural net \n",
    "\n",
    "#number of hidden layers\n",
    "n=4\n",
    "model= tf.keras.Sequential([tf.keras.layers.Dense(n),tf.keras.layers.Dense(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create sequential deep neural net \n",
    "\n",
    "#multiple number of hidden layers\n",
    "n1=4\n",
    "n2=3\n",
    "\n",
    "#number of output layers\n",
    "out=2\n",
    "\n",
    "#Deep Neural Net\n",
    "model= tf.keras.Sequential([tf.keras.layers.Dense(n1),\n",
    "                            tf.keras.layers.Dense(n2),\n",
    "                            tf.keras.layers.Dense(out)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important tf methods \n",
    "\n",
    "* loss=tf.reduce_mean(tf.square(tf.subtract(y,predict))) -- loss function for classication - probability values\n",
    "* loss=tf.keras.losses.MSE(y,predicted)  -- loss function for regression - continous value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent Algorithm - to train neural net \n",
    "* Initialize the wieghts to random \n",
    "* Calculate the empiral loss\n",
    "* loop untill convergence \n",
    "     gradient = derivative of empircal loss\n",
    "     update weights - \n",
    "        w=w-lr*gradient  - lr is learning rate the steps to move in search of optimal value\n",
    "        \n",
    "* Return weights\n",
    "\n",
    "#tensorflow code\n",
    "\n",
    "weights=tf.Variable([tf.random.normal()])\n",
    "\n",
    "while True:\n",
    "    with tf.GradientTape() as g:\n",
    "       loss= compute_loss(weights)\n",
    "       gradient= g.gradient(loss, weights)\n",
    "\n",
    "    weights= weights - lr*gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizating Gradient Descent\n",
    "\n",
    "* Using the different learning rates,plot and choose the optimal learning rate - feasible\n",
    "\n",
    "* Adaptive learning rate which uses \n",
    "   - how large the gradient is\n",
    "   - how fast learning is happening\n",
    "   - size of particular weights \n",
    "   \n",
    "Gradient Descent optimizers in tensorflow\n",
    "\n",
    "Algorithm      TF Implementation\n",
    "SGD            tf.keras.optimizers.SGD    - Stochastoc Gradient Model\n",
    "Adam           tf.keras.optimizers.Adam\n",
    "Adadelta       tf.keras.optimizers.Adadelta\n",
    "Adagrad        tf.keras.optimizers.Adagrad\n",
    "RMSProp        tf.keras.optimizers.RMSProp\n",
    "\n",
    "http://ruder.io/optimizing-gradient-descent/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Complete Deep Neural Network with Gradient Descent\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model=tf.keras.Sequential([...])\n",
    "\n",
    "#Pick your optimizer\n",
    "optimizer=tf.keras.optimizers.SGD()\n",
    "\n",
    "\n",
    "while True:\n",
    "     \n",
    "     #forward thru the network \n",
    "     prediction= model(x)\n",
    "     \n",
    "     with tf.GradientTape() as tape:\n",
    "         #compute the loss\n",
    "         loss= compute_loss(y,prediction)\n",
    "         \n",
    "    # update the weights using the gradient\n",
    "    grads= tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Regularization in the Neural Network \n",
    "\n",
    "Neural Networks tend to overfit easily as it learns the training data so efficiently and tends to behave bad \n",
    "over the test data or any new data.\n",
    "\n",
    "Underfit - Model is not trained completely both test and train performances badly\n",
    "Overfit  - Model performs well on the train data as it learned it so carefully any new data it just performs badly\n",
    "\n",
    "Ideal fit - When the model bias over variance there is a trade off here and this generalizes the model and has the capacity to\n",
    "            perform well on the test data and the train data. \n",
    "            \n",
    "How to handle the overfitting in the Neural Network:            \n",
    "1. Drop out method: Dropping of some of the activations randomly on every iteration \n",
    "   in which weights are learned randomly there by increasing its performance\n",
    "\n",
    "   tf.keras.layers.Dropout(p=0.5) - p is probability indicating percentage of activations(nodes) to be dropped \n",
    "\n",
    "2. Early stop - This method stops training when the empircal loss once increases on validation set in gradient descent method\n",
    "   Stop before we have chance to overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
